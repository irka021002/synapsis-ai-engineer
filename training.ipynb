{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import package yang dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "from time import time\n",
    "from PIL import Image\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter yang digunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "YOLOVERSION = \"yolov8s\"\n",
    "EPOCHS = 20\n",
    "PRETRAINED=True\n",
    "SIZE=640\n",
    "WORKERS=2\n",
    "BATCH=8\n",
    "THRESHOLD=.5\n",
    "# if os.path.exists('runs'):\n",
    "#     shutil.rmtree('runs')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model YOLOv8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('{}.pt'.format(YOLOVERSION))\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.2 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.192  Python-3.11.9 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=data.yaml, epochs=20, patience=50, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=2, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train\n",
      "Overriding model.yaml nc=80 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2118370  ultralytics.nn.modules.head.Detect           [6, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11137922 parameters, 11137906 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Irfan Kamal\\Desktop\\Tugas\\Synapsis AI Engineer\\train\\labels.cache... 2999 images, 97 backgrounds, 0 corrupt: 100%|██████████| 2999/2999 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\Irfan Kamal\\Desktop\\Tugas\\Synapsis AI Engineer\\train\\images\\Video1_223_jpg.rf.2ac13f8af55aa8cdaec31efc90e060ee.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\Irfan Kamal\\Desktop\\Tugas\\Synapsis AI Engineer\\train\\images\\Video1_223_jpg.rf.7b128ee386c613b286bba43e244eb776.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\Irfan Kamal\\Desktop\\Tugas\\Synapsis AI Engineer\\train\\images\\Video2_167_jpg.rf.9e4f70f84cfb904b66cfa73c7e0a4975.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\Irfan Kamal\\Desktop\\Tugas\\Synapsis AI Engineer\\train\\images\\Video2_167_jpg.rf.d83199aaf3ae0094677b3d7006c4bcb0.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Irfan Kamal\\Desktop\\Tugas\\Synapsis AI Engineer\\train\\labels.cache... 2999 images, 97 backgrounds, 0 corrupt: 100%|██████████| 2999/2999 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\Irfan Kamal\\Desktop\\Tugas\\Synapsis AI Engineer\\train\\images\\Video1_223_jpg.rf.2ac13f8af55aa8cdaec31efc90e060ee.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\Irfan Kamal\\Desktop\\Tugas\\Synapsis AI Engineer\\train\\images\\Video1_223_jpg.rf.7b128ee386c613b286bba43e244eb776.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\Irfan Kamal\\Desktop\\Tugas\\Synapsis AI Engineer\\train\\images\\Video2_167_jpg.rf.9e4f70f84cfb904b66cfa73c7e0a4975.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\Irfan Kamal\\Desktop\\Tugas\\Synapsis AI Engineer\\train\\images\\Video2_167_jpg.rf.d83199aaf3ae0094677b3d7006c4bcb0.jpg: 1 duplicate labels removed\n",
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/20      2.04G       1.27      1.613      1.255         71        640: 100%|██████████| 375/375 [01:22<00:00,  4.54it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:40<00:00,  4.66it/s]\n",
      "                   all       2999      14719       0.71      0.786      0.737      0.488\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/20      2.12G      1.201     0.9919      1.194         49        640: 100%|██████████| 375/375 [01:18<00:00,  4.77it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:41<00:00,  4.57it/s]\n",
      "                   all       2999      14719      0.715       0.84      0.752      0.497\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/20      2.13G      1.199     0.9511      1.185         37        640: 100%|██████████| 375/375 [01:19<00:00,  4.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:40<00:00,  4.62it/s]\n",
      "                   all       2999      14719      0.656      0.823      0.758      0.508\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/20      2.15G      1.177     0.9095      1.176         78        640: 100%|██████████| 375/375 [01:17<00:00,  4.83it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:39<00:00,  4.77it/s]\n",
      "                   all       2999      14719      0.723      0.886      0.788      0.536\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/20      2.13G      1.157      0.864      1.166         34        640: 100%|██████████| 375/375 [01:17<00:00,  4.83it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:39<00:00,  4.71it/s]\n",
      "                   all       2999      14719      0.737      0.854      0.785      0.543\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/20      2.14G      1.132      0.832      1.149         39        640: 100%|██████████| 375/375 [01:17<00:00,  4.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:39<00:00,  4.76it/s]\n",
      "                   all       2999      14719      0.736      0.892      0.796      0.557\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/20      2.14G      1.099     0.7881      1.129         31        640: 100%|██████████| 375/375 [01:17<00:00,  4.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:39<00:00,  4.76it/s]\n",
      "                   all       2999      14719      0.705      0.907      0.808      0.567\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/20      2.13G      1.092     0.7712      1.125         30        640: 100%|██████████| 375/375 [01:18<00:00,  4.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:39<00:00,  4.71it/s]\n",
      "                   all       2999      14719      0.686      0.939      0.809      0.572\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/20      2.13G      1.061     0.7372      1.112         49        640: 100%|██████████| 375/375 [01:18<00:00,  4.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:39<00:00,  4.77it/s]\n",
      "                   all       2999      14719      0.728      0.925      0.812      0.582\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/20      2.14G      1.042     0.7027      1.096         66        640: 100%|██████████| 375/375 [01:18<00:00,  4.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:39<00:00,  4.77it/s]\n",
      "                   all       2999      14719      0.741      0.939      0.821      0.589\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/20      2.13G      1.016     0.6569      1.093         27        640: 100%|██████████| 375/375 [01:17<00:00,  4.85it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:39<00:00,  4.73it/s]\n",
      "                   all       2999      14719      0.743       0.93      0.821      0.596\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/20      2.13G     0.9991     0.6385      1.084         30        640: 100%|██████████| 375/375 [01:17<00:00,  4.86it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:39<00:00,  4.77it/s]\n",
      "                   all       2999      14719      0.742      0.938      0.824      0.601\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/20      2.13G     0.9778     0.6114      1.074         38        640: 100%|██████████| 375/375 [01:16<00:00,  4.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:39<00:00,  4.79it/s]\n",
      "                   all       2999      14719      0.756      0.953      0.829      0.612\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/20      2.13G     0.9508     0.5825      1.055         29        640: 100%|██████████| 375/375 [01:17<00:00,  4.85it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:39<00:00,  4.75it/s]\n",
      "                   all       2999      14719       0.77       0.94       0.83      0.618\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/20      2.12G     0.9283     0.5641      1.041         30        640: 100%|██████████| 375/375 [01:16<00:00,  4.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:39<00:00,  4.78it/s]\n",
      "                   all       2999      14719      0.763      0.954      0.829      0.622\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/20      2.11G     0.9064     0.5457      1.032         60        640: 100%|██████████| 375/375 [01:17<00:00,  4.85it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:39<00:00,  4.75it/s]\n",
      "                   all       2999      14719      0.775      0.956      0.835      0.637\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/20      2.13G     0.8885     0.5275      1.021         28        640: 100%|██████████| 375/375 [01:17<00:00,  4.85it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:39<00:00,  4.79it/s]\n",
      "                   all       2999      14719       0.78      0.957      0.834      0.641\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/20      2.12G     0.8695     0.5083      1.004         27        640: 100%|██████████| 375/375 [01:17<00:00,  4.86it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:39<00:00,  4.77it/s]\n",
      "                   all       2999      14719       0.78      0.961      0.839      0.646\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/20      2.13G     0.8487     0.4931     0.9939         36        640: 100%|██████████| 375/375 [01:17<00:00,  4.83it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:40<00:00,  4.68it/s]\n",
      "                   all       2999      14719       0.78      0.966       0.84      0.653\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/20      2.13G     0.8274      0.478     0.9843         37        640: 100%|██████████| 375/375 [01:17<00:00,  4.85it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:39<00:00,  4.78it/s]\n",
      "                   all       2999      14719      0.783      0.966      0.843      0.659\n",
      "\n",
      "20 epochs completed in 0.665 hours.\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating runs\\detect\\train\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.192  Python-3.11.9 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "Model summary (fused): 168 layers, 11127906 parameters, 0 gradients, 28.4 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 188/188 [00:40<00:00,  4.62it/s]\n",
      "                   all       2999      14719      0.783      0.966      0.843      0.659\n",
      "               Goggles       2999        924      0.555      0.902      0.641      0.378\n",
      "                 boots       2999       4054      0.961      0.991      0.993      0.806\n",
      "                gloves       2999       4154       0.97      0.975      0.992      0.777\n",
      "                helmet       2999       2255      0.854      0.987      0.957      0.765\n",
      "                  mask       2999        506      0.498      0.964      0.512      0.385\n",
      "                  vest       2999       2826      0.858      0.977      0.962      0.842\n",
      "Speed: 0.2ms preprocess, 7.3ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([0, 1, 2, 3, 4, 5])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x000002130F426D90>\n",
       "fitness: 0.6772895926865625\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.37806,     0.80584,     0.77744,     0.76468,     0.38498,     0.84232])\n",
       "names: {0: 'Goggles', 1: 'boots', 2: 'gloves', 3: 'helmet', 4: 'mask', 5: 'vest'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': 0.782522589406628, 'metrics/recall(B)': 0.9661334667183982, 'metrics/mAP50(B)': 0.8429065142521401, 'metrics/mAP50-95(B)': 0.6588877125126094, 'fitness': 0.6772895926865625}\n",
       "save_dir: WindowsPath('runs/detect/train')\n",
       "speed: {'preprocess': 0.24169840467655884, 'inference': 7.316236457811987, 'loss': 0.0013353507413034161, 'postprocess': 1.0372360931630529}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(data='data.yaml', epochs = EPOCHS,\n",
    "                    batch=BATCH, pretrained=PRETRAINED,\n",
    "                    imgsz=SIZE, workers=WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percobaan deteksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(model, imgs, box_format='xyxy',th=0.5):\n",
    "    total_crops = []\n",
    "    total_boxes = []\n",
    "    total_scores = []\n",
    "    total_cls = []\n",
    "    \n",
    "    results = model.predict(imgs, stream=False, verbose=False)\n",
    "    \n",
    "    for i, image_result in enumerate(results):\n",
    "        filtered_crops = []\n",
    "        filtered_boxes = []\n",
    "        filtered_scores = []\n",
    "        filtered_cls = []\n",
    "        \n",
    "        img = imgs[i]\n",
    "        img_w, img_h = np.array(img).shape[:2]\n",
    "        \n",
    "        if box_format == 'xyxy':\n",
    "            image_boxes = image_result.boxes.xyxy.cpu().numpy()\n",
    "        elif box_format == 'xywh':\n",
    "            image_boxes = image_result.boxes.xyxy.cpu().numpy()\n",
    "            image_boxes = [[round(x1), round(y1), round(np.abs(x1-x2)), round(np.abs(y1-y2))] for x1,y1,x2,y2 in image_boxes]\n",
    "        elif box_format == 'xyxyn':\n",
    "            image_boxes = image_result.boxes.xyxyn.cpu().numpy()\n",
    "        elif box_format == 'xywhn':\n",
    "            image_boxes = image_result.boxes.xyxyn.cpu().numpy()\n",
    "            image_boxes = [[float(x1), float(y1), float(np.abs(x1-x2)), float(np.abs(y1-y2))] for x1,y1,x2,y2 in image_boxes]\n",
    "              \n",
    "        image_scores = image_result.boxes.conf.cpu().numpy()\n",
    "        image_cls = image_result.boxes.cls.cpu().numpy()\n",
    "        \n",
    "        for j in range(len(image_boxes)):\n",
    "            (x1, y1, x2, y2), score, c = image_boxes[j], image_scores[j], image_cls[j]\n",
    "            if score >= th:\n",
    "                filtered_scores.append(score)\n",
    "                filtered_cls.append(c)\n",
    "                if box_format == 'xyxy':\n",
    "                    filtered_crops.append(img.crop([x1,y1,x2,y2]))\n",
    "                    filtered_boxes.append([int(x1),int(y1),int(x2),int(y2)])\n",
    "                elif box_format == 'xyxyn':\n",
    "                    filtered_crops.append(img.crop([x1*img_w, y1*img_h, x2*img_w, y2*img_h]))\n",
    "                    filtered_boxes.append([float(x1),float(y1),float(x2),float(y2)])\n",
    "                elif box_format == 'xywh':\n",
    "                    filtered_crops.append(img.crop([x1,y1,x1+x2,y1+y2]))\n",
    "                    filtered_boxes.append([int(x1),int(y1),int(x2),int(y2)])\n",
    "                elif box_format == 'xywhn':\n",
    "                    filtered_crops.append(img.crop([x1*img_w,y1*img_h,(x1+x2)*img_w,(y1+y2)*img_h]))\n",
    "                    filtered_boxes.append([float(x1),float(y1),float(x2),float(y2)])\n",
    "                    \n",
    "        total_crops.append(filtered_crops)        \n",
    "        total_boxes.append(filtered_boxes)\n",
    "        total_scores.append(filtered_scores)\n",
    "        total_cls.append(filtered_cls)\n",
    "                \n",
    "\n",
    "    return total_crops, total_boxes, total_scores, total_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('./runs/detect/train/weights/best.pt')\n",
    "labels = ['Goggles', 'boots', 'gloves', 'helmet', 'mask', 'vest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"image_211_jpg.rf.d3903a253abcc558769bf013dd40d51c.jpg\")\n",
    "\n",
    "crops, boxes, scores, cls = detect(model, [Image.fromarray(image)], box_format='xywh',th=THRESHOLD)\n",
    "\n",
    "for (left, top, width, height), score, label in zip(boxes[0], scores[0], cls[0]):\n",
    "    color = (randrange(256), randrange(256), randrange(256))\n",
    "    cv2.rectangle(image, (int(left), int(top)), (int(left+width), int(top+height)+5), color, 2)\n",
    "    cv2.putText(image, labels[int(label)], (int(left), int(top)), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "cv2.imshow(\"Deteksi Gambar\", image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"Video4_280_jpg.rf.78840e232ee262d150d0965508765ebc.jpg\")\n",
    "\n",
    "crops, boxes, scores, cls = detect(model, [Image.fromarray(image)], box_format='xywh',th=THRESHOLD)\n",
    "\n",
    "for (left, top, width, height), score, label in zip(boxes[0], scores[0], cls[0]):\n",
    "    color = (randrange(256), randrange(256), randrange(256))\n",
    "    cv2.rectangle(image, (int(left), int(top)), (int(left+width), int(top+height)+5), color, 2)\n",
    "    cv2.putText(image, labels[int(label)], (int(left), int(top)), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "cv2.imshow(\"Deteksi Gambar\", image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"Video-Contoh-Trim.mp4\")\n",
    "while True:\n",
    "    ret, image = cap.read()\n",
    "    crops, boxes, scores, cls = detect(model, [Image.fromarray(image)], box_format='xywh',th=0.2)\n",
    "\n",
    "    for (left, top, width, height), score, label in zip(boxes[0], scores[0], cls[0]):\n",
    "        color = (randrange(256), randrange(256), randrange(256))\n",
    "        cv2.rectangle(image, (int(left), int(top)), (int(left+width), int(top+height)+5), color, 2)\n",
    "        cv2.putText(image, labels[int(label)], (int(left), int(top)), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "    image = cv2.resize(image, (1280, 720))\n",
    "    cv2.imshow(\"Deteksi Gambar\", image)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apabila dilihat dari video, tidak terdapat hasil dari model. Hal ini kemungkinan karena model tidak cukup bagus untuk melakukan deteksi pada jarak yang jauh. Solusi yang bisa dilakukan yaitu menambahkan dataset untuk kondisi jarak jauh seperti video, dan juga bisa dengan menggunakan model yang lebih besar seperti YOLOv8l ataupun YOLOv8x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
